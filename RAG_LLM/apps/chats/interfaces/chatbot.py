import streamlit as st
from apps.chats.models.vllm_models import load_qna_model
from apps.chats.prompts.classifier import classifier_system_prompt
from apps.chats.types.prompt import PromptRequest

from apps.chats.services.filter_rules import filtering_by_rules
from apps.chats.services.RAG import run_retriever
from apps.chats.services.llm_generator import generate_llm_response
from apps.chats.services.wordcloud_service import is_wordcloud_request, handle_wordcloud_request

from utils.sessions import initialize_session, show_messages, add_message
from utils.logger import logger

def main():
    st.title("Chat-bot")
    
    initialize_session()
    show_messages()
    
    if "tokenizer" not in st.session_state or "llm" not in st.session_state:
        with st.empty():
            st.markdown("ğŸš€ **ëª¨ë¸ì„ ì¤€ë¹„ ì¤‘ì…ë‹ˆë‹¤.**")
            with st.spinner("LLM ëª¨ë¸ì„ ì´ˆê¸° ë¡œë”© ì¤‘ì…ë‹ˆë‹¤."):
                tokenizer, llm = load_qna_model()
                st.session_state.tokenizer = tokenizer
                st.session_state.llm = llm
            st.success("LLM ëª¨ë¸ ì´ˆê¸°í™” ì™„ë£Œ")
    
    user_input = st.chat_input("ê¶ê¸ˆí•œ ë‚´ìš©ì„ ë¬¼ì–´ë³´ì„¸ìš”.")
    
    if not user_input:
        return
    
    st.chat_message("user").write(user_input)
    add_message("user", user_input)
    
    try:
        # wordcloud service
        if is_wordcloud_request(user_input=user_input):
            with st.spinner("ì›Œë“œí´ë¼ìš°ë“œ ìƒì„± ì¤‘"):
                handle_wordcloud_request(role="assistant")
            st.success("ì›Œë“œí´ë¼ìš°ë“œ ìƒì„± ì™„ë£Œ")
            return
        
        # filtering by rules (passed, reason, response, rule_type)
        filtered_result = filtering_by_rules(user_input=user_input)

        if not filtered_result.passed: # filtered: No RAG, No LLM
            if filtered_result.response: # ë‹µë³€ì´ ìˆì„ ì‹œ
                st.chat_message("assistant").write(response)
                add_message("assistant", response)
            st.warning(f"[ì˜¤ë¥˜] {reason}") # ì°¨ë‹¨ ë‚´ìš© ì„¤ëª…
            
            return # ë‹µë³€ ë
        
        # classifier
        classification_result = classifier_system_prompt(user_input=user_input)
        
        # RAG
        docs, context = run_retriever(user_input=user_input,
                                      classification_result=classification_result)
        
        # LLM
        tokenizer, llm = load_qna_model() # load model and save in cache
        
        request = PromptRequest(
            classification_result=classification_result,
            user_input=user_input,
            context=context,
            tokenizer=tokenizer,
            llm=llm
        )
        
        prompt, answer = generate_llm_response(request=request, docs=docs)
        st.chat_message("assistant").markdown(answer) # answer format in markdown
        add_message("assistant", answer)
        
    except Exception as e:
        logger.error(f"Error during debug response: {e}", exc_info=True)
        st.error("ì˜¤ë¥˜ê°€ ë°œìƒí–ˆìŠµë‹ˆë‹¤.")
        
if __name__ == "__main__":
    main()        
            